{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9686bbc5",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0595cdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model Accuracy: 0.6420000195503235\n",
      "Non-Linear Model Accuracy: 0.9020000100135803\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Generate toy circular dataset\n",
    "def generate_circular_data(n_samples=1000):\n",
    "    X = np.random.uniform(-1.5, 1.5, (n_samples, 2))\n",
    "    y = (X[:, 0]**2 + X[:, 1]**2 < 1).astype(int)  # 1 for inside, 0 for outside\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "X, y = generate_circular_data()\n",
    "\n",
    "# Linear Model (No activation)\n",
    "class LinearNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)  # 2 classes\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)  # No non-linear activation\n",
    "        return x\n",
    "\n",
    "# Non-Linear Model (ReLU)\n",
    "class NonLinearNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))  # Non-linear activation\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training function (simplified)\n",
    "def train_model(model, X, y, epochs=100):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model\n",
    "\n",
    "# Train both models\n",
    "linear_model = LinearNN()\n",
    "non_linear_model = NonLinearNN()\n",
    "train_model(linear_model, X, y)\n",
    "train_model(non_linear_model, X, y)\n",
    "\n",
    "# Evaluate accuracy (simplified)\n",
    "def get_accuracy(model, X, y):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = (predicted == y).float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "print(\"Linear Model Accuracy:\", get_accuracy(linear_model, X, y))    # Likely ~50-60%\n",
    "print(\"Non-Linear Model Accuracy:\", get_accuracy(non_linear_model, X, y))  # Likely ~90%+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb0f9f",
   "metadata": {},
   "source": [
    "Available Activations in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f591a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid: tensor([0.2689, 0.5000, 0.9526])\n",
      "ReLU: tensor([0., 0., 3.])\n",
      "Tanh: tensor([-0.7616,  0.0000,  0.9951])\n",
      "GELU: tensor([-0.1587,  0.0000,  2.9960])\n",
      "Leaky ReLU: tensor([-0.1000,  0.0000,  3.0000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example inputs\n",
    "x = torch.tensor([-1.0, 0.0, 3.0])\n",
    "\n",
    "# Common activation functions\n",
    "sigmoid = nn.Sigmoid()(x)  # [0.269, 0.5, 0.731]\n",
    "relu = nn.ReLU()(x)        # [0.0, 0.0, 1.0]\n",
    "tanh = nn.Tanh()(x)        # [-0.761, 0.0, 0.761]\n",
    "gelu = nn.GELU()(x)        # [-0.159, 0.0, 0.841]\n",
    "leaky_relu = nn.LeakyReLU(0.1)(x)  # [-0.1, 0.0, 1.0]\n",
    "\n",
    "print(\"Sigmoid:\", sigmoid)\n",
    "print(\"ReLU:\", relu)\n",
    "print(\"Tanh:\", tanh)\n",
    "print(\"GELU:\", gelu)\n",
    "print(\"Leaky ReLU:\", leaky_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3041e",
   "metadata": {},
   "source": [
    "## ReLU Activation Function Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a0b83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee8f4acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = []\n",
    "for i in inputs:\n",
    "    if i > 0:\n",
    "        outputs.append(i)\n",
    "    else:\n",
    "        outputs.append(0)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c2a64fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4304f3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = []\n",
    "for i in inputs:\n",
    "    outputs.append(max(i,0))\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df0c25",
   "metadata": {},
   "source": [
    "https://numpy.org/devdocs/reference/generated/numpy.maximum.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12b4a887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 2. , 0. , 3.3, 0. , 1.1, 2.2, 0. ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "output = np.maximum(0, inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595dfd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self,inputs):\n",
    "        # Calulator output values from \n",
    "        self.outputs =  np.maximum(0,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0797f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        self.weights =  0.01 * np.random.rand(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "856f66fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bed179e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19b9dbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "772c8bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c8801ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c925a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 2)\n",
      "(2, 3)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "dense1.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd5ff3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation1.forward(dense1.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9435605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 2.43705921e-06, 4.92539899e-05],\n",
       "       [8.20536938e-06, 2.86095750e-05, 1.79837876e-04],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [1.99606761e-04, 9.11498665e-05, 4.22687713e-04]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation1.outputs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a2c4c5",
   "metadata": {},
   "source": [
    "# The Softmax Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae195e1",
   "metadata": {},
   "source": [
    "<img src='./assets/formula_softmax.png' width=400  />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e588f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [4.8, 1.21, 2.385]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eff02c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# e - mathematical constant, we use E here to match a common coding\n",
    "# style where constants are uppercased\n",
    "E = 2.71828182846 # you can also use math.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be8184b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each value in a vector, calculate the exponential value\n",
    "exp_values = []\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(E ** output) # ** - power operator in Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5840bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values:\n",
      "[121.51041751893969, 3.3534846525504487, 10.85906266492961]\n"
     ]
    }
   ],
   "source": [
    "print('exponentiated values:')\n",
    "print(exp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a09dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_base = sum(exp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c106d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_values = []\n",
    "for value in exp_values:\n",
    "    norm_values.append(value / norm_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3702cd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized exponentiated values: [0.8952826639573506, 0.024708306782070668, 0.08000902926057876]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Normalized exponentiated values: {norm_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2bd6c3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of normalized values: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sum of normalized values: {sum(norm_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d043cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67c400a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [4.8, 1.21, 2.385]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b027d6ab",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/numpy-exp-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f62dd53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each value in a vector, calculate the exponential values\n",
    "exp_values = np.exp(layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "519bbfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponentiated values: [121.51041752   3.35348465  10.85906266]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Exponentiated values: {exp_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3573a597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized exponentiated values: [0.89528266 0.02470831 0.08000903]\n",
      "sum of normalized values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "# Normalized value\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "print(f'normalized exponentiated values: {norm_values}')\n",
    "print('sum of normalized values:', np.sum(norm_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967e136",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/numpy-sum-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0d2be0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = np.array([[4.8, 1.21, 2.385],\n",
    "                            [8.9, -1.81, 0.2],\n",
    "                            [1.41, 1.051, 0.026]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2e4804d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum without axis 18.172\n"
     ]
    }
   ],
   "source": [
    "print('Sum without axis', np.sum(layer_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fa3012a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will be identical to the above since default is None: 18.172\n"
     ]
    }
   ],
   "source": [
    "print('This will be identical to the above since default is None:',np.sum(layer_outputs, axis=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1ab84429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another way to think of it w/ a matrix == axis 0: columns: [15.11   0.451  2.611]\n"
     ]
    }
   ],
   "source": [
    "print('Another way to think of it w/ a matrix == axis 0: columns:', np.sum(layer_outputs, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f7b3c7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But we want to sum the rows instead, like this w/ raw py [8.395 7.29  2.487]\n"
     ]
    }
   ],
   "source": [
    "print(\"But we want to sum the rows instead, like this w/ raw py\", np.sum(layer_outputs, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2d4fde14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum axis 1, but keep the same dimension as input  [[8.395]\n",
      " [7.29 ]\n",
      " [2.487]]\n"
     ]
    }
   ],
   "source": [
    "print('Sum axis 1, but keep the same dimension as input ',np.sum(layer_outputs, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b9f3eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Acitvation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
    "        print(inputs - np.max(inputs,axis=1,keepdims=True))\n",
    "        print(f\"exp_values == {exp_values}\")\n",
    "        print(np.max(inputs,axis=1,keepdims=True))\n",
    "        # Normalized them for each example\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c41edaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = Acitvation_Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cd222d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2 -1  0]]\n",
      "exp_values == [[0.13533528 0.36787944 1.        ]]\n",
      "[[3]]\n"
     ]
    }
   ],
   "source": [
    "softmax.forward([[1,2,3]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
